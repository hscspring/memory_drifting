import torch
from transformers import AutoConfig

from kv import KVProjector


def verify_kv_projection(model_id):
    print(f"æ­£åœ¨åŠ è½½ {model_id} çš„é…ç½®...")
    config = AutoConfig.from_pretrained(model_id)
    
    # 2. æ£€æŸ¥å…³é”® GQA å‚æ•°
    num_layers = config.num_hidden_layers
    num_kv_heads = config.num_key_value_heads
    head_dim = config.hidden_size // config.num_attention_heads
    hidden_size = config.hidden_size
    
    print(f"æ¨¡å‹å±‚æ•°: {num_layers}")
    print(f"KV å¤´æ•° (GQA): {num_kv_heads}")
    print(f"æ¯ä¸ª Head çš„ç»´åº¦: {head_dim}")
    print("-" * 30)

    # 3. å®ä¾‹åŒ–ä½ çš„æŠ•å½±å™¨
    projector = KVProjector(config) # ä½¿ç”¨ä½ åˆšæ‰çš„ä»£ç é€»è¾‘
    
    # æ¨¡æ‹Ÿä¸€ä¸ª Batch çš„ M_t
    batch_size = 2
    m_t = torch.randn(batch_size, hidden_size)
    
    # 4. æ‰§è¡ŒæŠ•å½±
    virtual_pkv = projector(m_t)
    
    # 5. ç»´åº¦æ ¡éªŒé€»è¾‘
    assert len(virtual_pkv) == num_layers, "å±‚æ•°ä¸åŒ¹é…ï¼"
    
    for i, (k, v) in enumerate(virtual_pkv):
        # é¢„æœŸçš„å½¢çŠ¶: (batch, num_kv_heads, seq_len=1, head_dim)
        expected_shape = (batch_size, num_kv_heads, 1, head_dim)
        
        if k.shape != expected_shape or v.shape != expected_shape:
            print(f"âŒ ç¬¬ {i} å±‚å½¢çŠ¶é”™è¯¯ï¼")
            print(f"   é¢„æœŸ: {expected_shape}")
            print(f"   å®é™…: K={tuple(k.shape)}, V={tuple(v.shape)}")
            return
    
    print("âœ… ç»´åº¦éªŒè¯é€šè¿‡ï¼æ‰€æœ‰æ³¨å…¥çš„ KV ç¼“å­˜ä¸ Qwen2.5 GQA æ¶æ„å®Œç¾å¯¹é½ã€‚")
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in projector.parameters())
    print(f"ğŸ“Š KVProjector æ€»å‚æ•°é‡: {total_params / 1e6:.2f} M (ä»…å  7B æ¨¡å‹çš„çº¦ 0.1%)")

# æ‰§è¡ŒéªŒè¯
# (ç¡®ä¿ QwenKVProjector ç±»å·²åœ¨ä¸Šæ–¹å®šä¹‰)
model_id = "/backup/lanzhenzhongLab/public/models/Qwen2.5-7B-Instruct"
verify_kv_projection(model_id)