# README


```bash
# 数据生成
python data_creator.py openrouter/google/gemini-2.5-pro

# 校验重复性+合并
python check_and_combine_ds.py
# 校验最终label
python check_labeled.py

# 评测
python eval_local.py
python eval_close.py

# 先生成再评测
python eval_offline.py
```



We introduce a learnable, writable attention anchor by injecting a latent memory directly into the KV-cache of a frozen LLM.


In CASS, the state vector Mt does not serve as an explicit memory container. Instead, it functions as a differentiable state carrier that anchors the update dynamics. The learned initialization M0 represents a meta-state indicating readiness for fact assimilation, rather than encoding any factual content.

Although the read embedding is prepended in the embedding sequence, it does not occupy any temporal position in the autoregressive sense, as no positional encoding is applied to it. Therefore, it acts as a global conditioning query rather than a sequence token.

Decouple temporal continuity from positional encoding by storing dialogue state in attention-space memory (KV), while using position_ids only for intra-turn ordering.

We reinterpret the KV cache not as a growing buffer of past token representations, but as a constant-sized, state-conditioned attention interface. The cache is generated by an external recurrent state estimator and injected into each Transformer layer, enabling O(1) memory access while remaining fully compatible with the original attention mechanism.




关于位置：

凡是“机制性 conditioning”，不进入 position system  
凡是“序列语义”，才使用 position system

我们不是在让模型“记住 token”，而是在训练一个网络，把对话状态直接映射成Transformer 每一层最有用的中间表示——KV Cache。
一旦 KV Cache 被构造出来，Transformer 就会在 attention 计算中天然地使用它，无需显式回放历史文本。
构造了一个新的、状态驱动的 KV 源，并把它接到了 Transformer 原本的 attention 入口上。



During inference, the slot selector is not explicitly applied.
Memory selection is implicitly performed by the LLM’s attention over the injected KV cache.
The selector is trained only as an auxiliary supervision signal to shape memory representations for effective attention-based retrieval.